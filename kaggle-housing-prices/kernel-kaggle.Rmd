---
title: 'Exploratory analyis and machine learning model for predicting housing prices with R (data.table and caret)'
date: "`r format(Sys.Date())`"
author: "Bart Boerman"
output:
  html_document:
    theme: lumen
    number_sections: false
    font-family: Open Sans, sans-serif
    font-import: https://fonts.googleapis.com/css?family=Open+Sans
    code_folding: show
    highlight: tango
editor_options: 
  chunk_output_type: console
---
<style type="text/css">

body{ /* Normal  */
      font-size: 14px;
  }
td {  /* Table  */
  font-size: 12px;
}
h1, .h1, h2, .h2, h3, .h3 {
    margin-top: 10.5px;
    margin-bottom: 10.5px;
}
h1.title {
  font-size: 28px;
  color: #7db956;
}
h1 { /* Header 1 */
  font-size: 28px;
  color: #3e4a52;
}
h2 { /* Header 2 */
    font-size: 18px;
  color: #3e4a52;
}
h3 { /* Header 3 */
  font-size: 14px;
  color: #3e4a52;
}
code.r{ /* Code block */
    font-size: 12px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 14px;
}
th.sorting { /* DT column headers  */
    text-align: left;
}
</style>
**Raw inital version**.

# Introduction

Exploratory analyis and machine learning modal for predicting 
housing prices in competition ["House Prices: Advanced Regression Techniques"](https://www.kaggle.com/c/house-prices-advanced-regression-techniques). The aim is to predict house prices based on the provided data:

- **train.csv**,data for training our model 
- **test.csv**, data used to see how well our model performs on unseen data

In addition I want to gain and share some basic knowledge of data wrangling and analysis with data.table.

# Required libraries

```{r setup, message=FALSE, warning=FALSE}
require(knitr)              ## dynamic report generation in R
require(DT)                 ## display data in html tables
require(ggplot2)            ## plotting 
require(gridExtra)          ## Arrange visualizations using grid 
require(data.table)         ## fast data wrangling and analysis
require(psych)              ## descriptive statistics, skewness and kurtosis
require(caret)              ## (near) zero variance, train and predict
require(caretEnsemble)      ## ensemble modelling
require(xgboost)
require(glmnet)
knitr::opts_chunk$set(error=FALSE, message=FALSE, warning=FALSE) ## some defaults for this report
```

# Get data into R

```{r}
train.dt <- fread(input = "train.csv", 
                  sep = ",", 
                  nrows = -1,
                  header = T,
                  na.strings=c("NA","N/A","null"),
                  stringsAsFactors = F,
                  check.names = T,
                  strip.white = T,
                  blank.lines.skip = T,
                  data.table = T
) 
test.dt <- fread(input = "test.csv", 
                 sep = ",", 
                 nrows = -1,
                 header = T,
                 na.strings=c("NA","N/A","null"),
                 stringsAsFactors = F,
                 check.names = T,
                 strip.white = T,
                 blank.lines.skip = T,
                 data.table = T
) 
## Create one data set for feature engineering. 
train.dt[, dataPartition:="train"]
test.dt[, SalePrice:=as.integer(NA)] 
test.dt[, dataPartition:="test"]
full.dt <- rbindlist(list(train.dt, test.dt), use.names = F, fill = F)
```


# Data understanding

A description of the data is available on [Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data). In the next block of code I've grouped the variables (data items) in three groups: square footage, values and factors. These groups will be used for engineering on the complete group rather than specifying one transformation at a time.

```{r}
## Numeric, square footage
variablesSquareFootage <- c(
  "LotFrontage", 		## Linear feet of street connected to property 
  "LotArea",    		## Lot size in square feet
  "MasVnrArea",  		## Masonry veneer area in square feet
  "BsmtFinSF1",		  ## Type 1 finished square feet	
  "BsmtFinSF2",		  ## Type 2 finished square feet
  "BsmtUnfSF",		  ## Unfinished square feet of basement area
  "TotalBsmtSF", 		## Total square feet of basement area
  "FirstFlrSF",		  ## First Floor square feet
  "SecondFlrSF",	  ## Second floor square feet
  "LowQualFinSF", 	## Low quality finished square feet (all floors)
  "GrLivArea", 		  ## Above grade (ground) living area square feet
  "GarageArea",     ## Size of garage in square feet
  "WoodDeckSF",     ## Wood deck area in square feet
  "OpenPorchSF",    ## Open porch area in square feet  
  "EnclosedPorch",  ## Enclosed porch area in square feet 
  "ThreeSsnPorch",  ## Three season porch area in square feet 
  "ScreenPorch",    ## Screen porch area in square feet
  "PoolArea" 		    ## Pool area in square feet
)
## Counts, a house has n of something
variablesCounts <- c(
  "BsmtFullBath",		## Basement full bathrooms
  "BsmtHalfBath",		## Basement half bathrooms
  "FullBath",			  ## Full bathrooms above grade
  "HalfBath",			  ## Half baths above grade
  "BedroomAbvGr",		## Bedrooms above grade (does NOT include basement bedrooms)
  "KitchenAbvGr",		## Kitchens above grade
  "TotRmsAbvGrd",		## Total rooms above grade (does not include bathrooms)
  "Fireplaces",		  ## Number of fireplaces
  "GarageCars"     	## Size of garage in car capacity
)
## Values
variablesValues <- c(
  "MiscVal",        ## $ Value of miscellaneous feature
  "SalePrice"       ## $ Price paid
)
## Factors
variablesFactor <- colnames(full.dt)[which(as.vector(full.dt[,sapply(full.dt, class)]) == "character")]
variablesFactor <- setdiff(variablesFactor, "dataPartition") 
variablesFactor <- c(variablesFactor,
                     ## variables with data type integer which are factors
                     "MSSubClass",     ## Identifies the type of dwelling involved in the sale
                     "OverallQual",    ## Rates the overall material and finish of the house
                     "OverallCond"     ## Rates the overall condition of the house
)
```

## Data engineering

Before further data exploration we need to engineer the data to the desired format. Doing so makes sure that  the data types are correct and ordinal factors are sorted in the right order. The code performs the following tasks: rename variable names, change data type to factor and order ordinal factors. 

R does not support numbers as first character of variable names. Some variables where prefixed with "X" when the data was fetched.  

```{r}
setnames(full.dt, c("X1stFlrSF","X2ndFlrSF","X3SsnPorch"), c("FirstFlrSF","SecondFlrSF","ThreeSsnPorch"))
```

Before we go further we need to do some housekeeping on the data, like fixing typo's or spaces/& in categories (factor levels). 

```{r}
full.dt[GarageYrBlt == 2207, GarageYrBlt:= 2007] ## Fix typo
full.dt[MSSubClass  == 150, MSSubClass:= 160] ## 150 not in training set
full.dt[Exterior1st  == "Wd Sdng", Exterior1st:= "WdSdng"] ## Fix spaces
full.dt[Exterior2nd  == "Wd Sdng", Exterior2nd:= "WdSdng"] ## Fix spaces
full.dt[Exterior2nd  == "Brk Cmn", Exterior2nd:= "BrkComm"] ## Fix typo
full.dt[Exterior2nd  == "Wd Shng", Exterior2nd:= "WdShing"] ## Fix typo
full.dt[RoofMatl  == "Tar&Grv", RoofMatl:= "TarGrv"] ## Fix '&'
full.dt[RoofMatl  == "WdShngl", RoofMatl:= "WdShing"] ## See exterior
```

Imputation of missing values is part of the data cleansing process. We will do this task after analyzing descriptive statistics. 

Since categorical variables enter into statistical models differently than continuous variables, storing data as factors insures that the modeling functions will treat such data correctly.

```{r}
changeColType <- variablesFactor
full.dt[,(changeColType):= lapply(.SD, as.factor), .SDcols = changeColType]
## Set columns to numeric
changeColType <- c(variablesSquareFootage, variablesCounts, variablesValues)
full.dt[,(changeColType):= lapply(.SD, as.numeric), .SDcols = changeColType]
```

An ordered factor is used to represent an ordinal variable.

```{r}
## OverallQual, rates the overall material and finish of the house
full.dt[,OverallQual:=ordered(OverallQual, levels = c(1:10))]
## OverallCond, rates the overall condition of the house
full.dt[,OverallCond:=ordered(OverallCond, levels = c(1:10))]
## KitchenQual, kitchen quality
full.dt[,KitchenQual:=ordered(KitchenQual, levels = c("Po","Fa","TA","Gd","Ex"))]
## GarageFinish (contains NA's)
full.dt[,GarageFinish:=ordered(GarageFinish, levels = c("None","Unf","RFn","Fin"))]
## GarageQual
full.dt[,GarageQual:=ordered(GarageQual, levels = c("None","Po","Fa","TA","Gd","Ex"))]
## GarageCond
full.dt[,GarageCond:=ordered(GarageCond, levels = c("None","Po","Fa","TA","Gd","Ex"))]
## ExterQual, evaluates the quality of the material on the exterior  
full.dt[,ExterQual:=ordered(ExterQual, levels = c("Po","Fa","TA","Gd","Ex"))]
## ExterCond, evaluates the present condition of the material on the exterior
full.dt[,ExterCond:=ordered(ExterCond, levels = c("Po","Fa","TA","Gd","Ex"))]
## BsmtQual (contains NA's), evaluates the height of the basement
full.dt[,BsmtQual:=ordered(BsmtQual, levels = c("None","Po","Fa","TA","Gd","Ex"))]
## BsmtCond (contains NA's), evaluates the general condition of the basement
full.dt[,BsmtCond:=ordered(BsmtCond, levels = c("None","Po","Fa","TA","Gd","Ex"))]
## BsmtExposure (contains NA's), refers to walkout or garden level walls
full.dt[,BsmtExposure:=ordered(BsmtExposure, levels = c("None","No","Mn","Av","Gd"))]
## BsmtFinType1 (contains NA's), rating of basement finished area
full.dt[,BsmtFinType1:=ordered(BsmtFinType1, levels = c("None","Unf","LwQ","Rec","BLQ","ALQ","GLQ"))]
## FireplaceQu (contains NA's), fireplace quality
full.dt[,FireplaceQu:=ordered(FireplaceQu, levels = c("None","Po","Fa","TA","Gd","Ex"))]
## Electrical
full.dt[,Electrical:=ordered(Electrical, levels = c("FuseP","Mix","FuseF","FuseA","SBrkr"))]
## Fence
full.dt[,Fence:=ordered(Fence, levels = c("None","MnWw","MnPrv","GdWo","GdPrv"))]
## PoolQC
full.dt[,PoolQC:=ordered(PoolQC, levels = c("None","Fa","Gd","Ex"))]
```

# Descriptive statistics

Descriptive statistics describe quantitatively the basic features of the data. These statistics will give us a head start by providing information about stuff like skewness, outliers (range) missing data points and (near) zero variance.

# Impute missing values

```{r}
#### function to find the mode.
findMode <- function(x) {
  names(table(x))[table(x)==max(table(x))]
}
#### imputations
## Kitchen
full.dt[is.na(KitchenQual), KitchenQual := findMode(full.dt$KitchenQual) ] ## One record, set to Typical
## Garage
full.dt[is.na(GarageFinish) & GarageType == "Detchd", ':=' (GarageFinish = "Fin",
                                                            GarageCars = 1,
                                                            GarageArea = 360,
                                                            GarageYrBlt = YearBuilt,
                                                            GarageQual = findMode(full.dt$GarageQual),
                                                            GarageCond = findMode(full.dt$GarageCond))] 
full.dt[is.na(GarageFinish), GarageFinish := "None"]
full.dt[is.na(GarageQual), GarageQual := "None"]
full.dt[is.na(GarageCond), GarageCond := "None"]
full.dt[is.na(GarageType), GarageType := "None"]
full.dt[is.na(GarageYrBlt), GarageYrBlt := 0]
## Basement
full.dt[is.na(BsmtExposure) & BsmtFinType1 == "Unf" , BsmtExposure := "No"]
full.dt[is.na(BsmtExposure), BsmtExposure := "None"]
full.dt[is.na(BsmtQual) & BsmtFinType1 == "Unf" , BsmtQual := "TA"]
full.dt[is.na(BsmtQual), BsmtQual := "None"]
full.dt[is.na(BsmtCond), BsmtCond := "None"]
full.dt[is.na(BsmtFinType1), BsmtFinType1 := "None"]
full.dt[is.na(BsmtFinType2) & BsmtFinSF2 > 0, BsmtFinType2 := "Unf"]
full.dt[is.na(BsmtFinType2), BsmtFinType2 := "None"]
full.dt[is.na(BsmtFinSF1),':=' (BsmtFinSF1 = 0, BsmtFinSF2 = 0, BsmtUnfSF = 0, TotalBsmtSF = 0)] 
full.dt[is.na(BsmtFullBath),':=' (BsmtFullBath = 0, BsmtHalfBath = 0)] 
## FireplaceQu  
full.dt[is.na(FireplaceQu), FireplaceQu := "None"]
## MSZoning
## RL for missing MSZoning in Mitchel because GrLivArea is greater then max of RM
## Not sure (yet) for missing MSZoning in IDOTRR. RM is most common in IDOTRR but might be wrong
full.dt[is.na(MSZoning) & Neighborhood == "Mitchel", MSZoning := "RL"]
full.dt[is.na(MSZoning) & Neighborhood == "IDOTRR", MSZoning  := "RM"]
## Electrical
## Most common value for neighborhood Timber is SBrkr
full.dt[is.na(Electrical) , Electrical  := findMode(full.dt$Electrical)]
## Exterior
## Most common for neighborhood and large total square footage is "MetalSd"
full.dt[is.na(Exterior1st),':=' (Exterior1st = findMode(full.dt$Exterior1st),Exterior2nd = findMode(full.dt$Exterior2nd))]
## MasVnrType and MasVnrArea. Taking the easy way out here
full.dt[is.na(MasVnrType),':=' (MasVnrType = "None", MasVnrArea = 0)]
## SaleType
full.dt[is.na(SaleType), SaleType := findMode(full.dt$SaleType)]
## Functional
full.dt[is.na(Functional), Functional := findMode(full.dt$Functional)]
## MiscFeature
full.dt[is.na(MiscFeature), MiscFeature := "None"]
## Alley
full.dt[is.na(Alley), Alley := "None"]
## Utilities
full.dt[is.na(Utilities), Utilities := findMode(full.dt$Utilities)]
## PoolQC
full.dt[is.na(PoolQC), PoolQC := "None"]
## Fence
full.dt[is.na(Fence), Fence := "None"]
## LotFrontage
## Alternative 1, impute by the median per neigborhood 
# full.dt[, LotFrontage := replace(LotFrontage, is.na(LotFrontage), median(LotFrontage, na.rm=TRUE)), by=.(Neighborhood)]
## Alternatove 2, impute with logistic regression
fit <- lm(log1p(LotFrontage) ~ log1p(LotArea) + LotConfig, data = full.dt[!is.na(LotFrontage),])
full.dt[is.na(LotFrontage), LotFrontage :=  round(expm1(predict(fit, newdata = full.dt[is.na(LotFrontage),])),0 )]
```

# Visualize key aspects of data

To be included later.

# Correlation

```{r}
variablesNumeric <- sapply(full.dt,is.numeric) ## both numeric integers
corrData <- full.dt[dataPartition == "train", ..variablesNumeric ]
corMatrix = cor(corrData)
#### table with highly correlated values
corTable <- setDT(melt(corMatrix))[order(-value)][value!=1] 
corTableSalePrice <- corTable[Var1=="SalePrice",][order(-value)]
corTableHigh <- corTable[value > 0.8 | value < -0.80 ][order(Var1,-value)]
```

```{r}
variablesHighlyCorrelated <- c(
  "GarageCond",     ## high correlation with GarageQual
  "GarageYrBlt",    ## high correlation with GarageQual
  "GarageArea",     ## high correlation GarageCars
  "TotRmsAbvGrd" ,  ## high correlation GrLivArea
  "YearRemodAdd",   ## correlated with qualities and age 
  "TotalBsmtSF",      ## correlated	with BsmtCond BsmtQual
  "BsmtFinSF1"       ## correlated with 	BsmtFinType1
)
```

# Feature engineering

```{r}
houseStyle.bin <- c("1Story" = "1Story", 
                     "1.5Fin" = "1.5Story", 
                     "1.5Unf" = "1.5Story",
                     "2.5Unf" = "2.5Story",
                     "2.5Fin" = "2.5Story",
                     "2Story" = "2Story",
                     "SFoyer" = "SFoyer",
                     "SLvl" = "SLvl") 
full.dt[, ':=' (
                age = YrSold - YearRemodAdd,
                isRemodeled = ifelse(YearRemodAdd == YearBuilt, 1, 0),
                isNew       = ifelse(YrSold       == YearBuilt, 1, 0),
                overallQualGood    = ifelse(as.integer(OverallQual) - 5 < 0, 0, as.integer(OverallQual) - 5),
                overallQualBad     = ifelse(5 - as.integer(OverallQual) < 0, 0, 5 - as.integer(OverallQual)),
                sfPorch = EnclosedPorch + ThreeSsnPorch + ScreenPorch,
                sfTotal     = (TotalBsmtSF + FirstFlrSF + SecondFlrSF),  
                hasUnfinishedLevel= ifelse(HouseStyle %in% c("1.5Unf","2.5Unf"),1,0),
                HouseStyle = as.factor(houseStyle.bin[HouseStyle]),
                countBathrooms = FullBath + HalfBath + BsmtHalfBath + BsmtFullBath,
                averageRoomSizeAbvGrd = GrLivArea / TotRmsAbvGrd,
                bathRoomToRoomAbvGrd = (FullBath + HalfBath) / TotRmsAbvGrd,
                landscapeInteraction = as.integer(LotShape) * as.integer(LandContour),
                garageInteraction = GarageCars * as.integer(GarageQual),
                yrMoSoldInt = as.numeric(format(as.Date(paste(YrSold, MoSold, "1", sep="-")), '%Y%m')) 
)]

```

# Data preparation

```{r}
#### select features
variablesDrop <- NA
response <- "SalePrice"  
features <- setdiff(names(full.dt), c(response,variablesDrop, variablesHighlyCorrelated, "Id","dataPartition"))
```

```{r}
##### remove outliers
outliersHigh.Id <-  train.dt[GrLivArea > 4000 | LotArea > 100000 | X1stFlrSF > 3000 | GarageArea > 1200,Id]
full.dt <- full.dt[!(Id %in% outliersHigh.Id)]
```

```{r}
#### create index for to split full data set into train and test
setkey(full.dt,dataPartition)
train.full.dt <- full.dt["train"]
```

```{r}
#### ordinal factors
## convert ordinal factors to integers. It gives a small performance boost for glmnet and xgbTree.
changeColType <- setDT(data.frame(sapply(full.dt,is.ordered)), keep.rownames = TRUE)[sapply.full.dt..is.ordered.==TRUE]$rn
full.dt[,(changeColType):= lapply(.SD, as.integer), .SDcols = changeColType]
```

```{r}
#### skewed variables
## log transform skewed variables (including response variable)
skewedVariables <- sapply(full.dt[, c(variablesSquareFootage,variablesValues), with = FALSE],function(x){skew(x,na.rm=TRUE)})
## keep only features that exceed a threshold for skewness
skewedVariables <- skewedVariables[skewedVariables > 0.50]
## transform excessively skewed features with log
skewedVariables <- names(skewedVariables)
full.dt[, (skewedVariables) := lapply(.SD, function(x) log1p(x)), .SDcols = skewedVariables]
```

```{r}
#### scale
varScale <- setdiff(c(variablesSquareFootage, variablesValues), c(response)) ## Do not scale response
full.dt[, (varScale) := lapply(.SD, function(x) scale(x, center = T, scale = T)), .SDcols = varScale]
```

```{r}
## split in train and test after engineering. Split by key is fasted method.
train.full.dt <- full.dt["train"]
test.dt <- full.dt["test"]
## random split training into train and validate
set.seed(333)
n <- nrow(train.full.dt)
shuffled.dt <- train.full.dt[sample(n), ]
train_indices <- 1:round(0.8 * n)
train.dt <- shuffled.dt[train_indices, ]
validate_indices <- (round(0.8 * n) + 1):n
validate.dt <- shuffled.dt[validate_indices, ]
```

# Modeling

```{r}
formula.all <- as.formula(paste("SalePrice ~ ", paste(features, collapse= "+")))
trControl <- trainControl(
        method="cv",
        number=7,
        savePredictions="final",
        index=createResample(train.dt$OverallQual, 7),  
        allowParallel =TRUE
)
glmnetGridElastic <- expand.grid(.alpha = 0.3, .lambda = 0.009) ## notice the . before the parameter
glmnetGridLasso <- expand.grid(.alpha = 1, .lambda = seq(0.001,0.1,by = 0.001))
glmnetGridRidge <- expand.grid(.alpha = 0, .lambda = seq(0.001,0.1,by = 0.001))
xgbTreeGrid <- expand.grid(nrounds = 400, max_depth = 3, eta = 0.1, gamma = 0, colsample_bytree = 1.0,  subsample = 1.0, min_child_weight = 4)
set.seed(333)
modelList <- caretList(
                  formula.all, 
                  data=train.dt,
                  trControl=trControl,
                  metric="RMSE",
                  tuneList=list(
                  ## Do not use custom names in list. Will give prediction error with greedy ensemble. Bug in caret.
                          xgbTree = caretModelSpec(method="xgbTree",  tuneGrid = xgbTreeGrid, nthread = 8),
                          lm =   caretModelSpec(method="lm"),
                          glmnet=caretModelSpec(method="glmnet", tuneGrid = glmnetGridElastic), ## Elastic 
                          glmnet=caretModelSpec(method="glmnet", tuneGrid = glmnetGridLasso), ## Lasso
                          glmnet=caretModelSpec(method="glmnet", tuneGrid = glmnetGridRidge) ## Ridge
                          )
)
```

# Evaluation

```{r}
xyplot(resamples(modelList))
modelCor(resamples(modelList))
summary(resamples(modelList))
varImp(modelList$xgbTree)
varImp(modelList$glmnet)
```

```{r}
#### residual analysis
plot(train.dt$OverallQual, resid(modelList$glmnet), 
       ylab="Residuals", xlab="Neigborhood", 
       main="") 
       abline(0, 0)                  
     
qqnorm(resid(modelList$glmnet), 
                ylab="Standardized Residuals", 
                xlab="Normal Scores", 
                main="Q-Q plot residuals") 
       qqline(resid(modelList$glmnet))
```

# Ensemble

```{r}
set.seed(333)
greedyEnsemble <- caretEnsemble(
  modelList, 
  metric="RMSE",
  trControl=trainControl(
    number=7, method = "cv"
  ))

summary(greedyEnsemble)
```
# Predict and submit results

```{r}
#### submit
finalPredictions <- predict(greedyEnsemble, newdata=test.dt)
finalPredictions <- data.frame(finalPredictions)
names(finalPredictions) <- "SalePrice"
finalPredictions$SalePrice <- expm1(finalPredictions$SalePrice) 
submission <- cbind(test.dt[, "Id"],finalPredictions)
write.csv(x = submission, file = "submission.csv", row.names = F)
```



